{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import random\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reweight distribution\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "def loadFile():\n",
    "    text = open('book.txt').read().lower()\n",
    "    return text\n",
    "\n",
    "text = loadFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 30798\n",
      "Unique characters: 40\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data\n",
    "\n",
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('Number of sequences:', len(sentences))\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sample function\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30798/30798 [==============================] - 16s 526us/step - loss: 2.5258\n",
      "Epoch 2/100\n",
      "30798/30798 [==============================] - 15s 503us/step - loss: 2.0510\n",
      "Epoch 3/100\n",
      "30798/30798 [==============================] - 16s 507us/step - loss: 1.8616\n",
      "Epoch 4/100\n",
      "30798/30798 [==============================] - 15s 503us/step - loss: 1.7305\n",
      "Epoch 5/100\n",
      "30798/30798 [==============================] - 16s 524us/step - loss: 1.6227\n",
      "Epoch 6/100\n",
      "30798/30798 [==============================] - 16s 519us/step - loss: 1.5282\n",
      "Epoch 7/100\n",
      "30798/30798 [==============================] - 16s 531us/step - loss: 1.4471\n",
      "Epoch 8/100\n",
      "30798/30798 [==============================] - 16s 515us/step - loss: 1.3447\n",
      "Epoch 9/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 1.3145\n",
      "Epoch 10/100\n",
      "30798/30798 [==============================] - 16s 505us/step - loss: 1.2617\n",
      "Epoch 11/100\n",
      "30798/30798 [==============================] - 16s 532us/step - loss: 1.2129\n",
      "Epoch 12/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 1.1668\n",
      "Epoch 13/100\n",
      "30798/30798 [==============================] - 17s 537us/step - loss: 1.1360\n",
      "Epoch 14/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 1.1073\n",
      "Epoch 15/100\n",
      "30798/30798 [==============================] - 16s 516us/step - loss: 1.0869\n",
      "Epoch 16/100\n",
      "30798/30798 [==============================] - 16s 527us/step - loss: 1.0598\n",
      "Epoch 17/100\n",
      "30798/30798 [==============================] - 16s 533us/step - loss: 1.0421\n",
      "Epoch 18/100\n",
      "30798/30798 [==============================] - 15s 503us/step - loss: 1.0217\n",
      "Epoch 19/100\n",
      "30798/30798 [==============================] - 16s 526us/step - loss: 0.9949\n",
      "Epoch 20/100\n",
      "30798/30798 [==============================] - 16s 521us/step - loss: 0.9873\n",
      "Epoch 21/100\n",
      "30798/30798 [==============================] - 16s 505us/step - loss: 0.9718\n",
      "Epoch 22/100\n",
      "30798/30798 [==============================] - 16s 505us/step - loss: 0.9627\n",
      "Epoch 23/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.9524\n",
      "Epoch 24/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.9342\n",
      "Epoch 25/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.9260\n",
      "Epoch 26/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.9158\n",
      "Epoch 27/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.9099\n",
      "Epoch 28/100\n",
      "30798/30798 [==============================] - 16s 507us/step - loss: 0.8953\n",
      "Epoch 29/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.8807\n",
      "Epoch 30/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.8702\n",
      "Epoch 31/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.8514\n",
      "Epoch 32/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.8395\n",
      "Epoch 33/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.8318\n",
      "Epoch 34/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.8116\n",
      "Epoch 35/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.8114\n",
      "Epoch 36/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.7971\n",
      "Epoch 37/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.7900\n",
      "Epoch 38/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.7706\n",
      "Epoch 39/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.7685\n",
      "Epoch 40/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.7625\n",
      "Epoch 41/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.7534\n",
      "Epoch 42/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.7487\n",
      "Epoch 43/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.7376\n",
      "Epoch 44/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.7304\n",
      "Epoch 45/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.7192\n",
      "Epoch 46/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.7073\n",
      "Epoch 47/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.7065\n",
      "Epoch 48/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.6939\n",
      "Epoch 49/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.6917\n",
      "Epoch 50/100\n",
      "30798/30798 [==============================] - 16s 517us/step - loss: 0.6879\n",
      "Epoch 51/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.6826\n",
      "Epoch 52/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.6804\n",
      "Epoch 53/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.6682\n",
      "Epoch 54/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.6619\n",
      "Epoch 55/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.6476\n",
      "Epoch 56/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.6520\n",
      "Epoch 57/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.6404\n",
      "Epoch 58/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.6324\n",
      "Epoch 59/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.6297\n",
      "Epoch 60/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.6181\n",
      "Epoch 61/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.6163\n",
      "Epoch 62/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.6088\n",
      "Epoch 63/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.6036\n",
      "Epoch 64/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.5940\n",
      "Epoch 65/100\n",
      "30798/30798 [==============================] - 16s 514us/step - loss: 0.5857\n",
      "Epoch 66/100\n",
      "30798/30798 [==============================] - 16s 517us/step - loss: 0.5846\n",
      "Epoch 67/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.5753\n",
      "Epoch 68/100\n",
      "30798/30798 [==============================] - 16s 518us/step - loss: 0.5691\n",
      "Epoch 69/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.5677\n",
      "Epoch 70/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.5661\n",
      "Epoch 71/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.5596\n",
      "Epoch 72/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.5499\n",
      "Epoch 73/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.5495\n",
      "Epoch 74/100\n",
      "30798/30798 [==============================] - 16s 516us/step - loss: 0.5346\n",
      "Epoch 75/100\n",
      "30798/30798 [==============================] - 16s 509us/step - loss: 0.5344\n",
      "Epoch 76/100\n",
      "30798/30798 [==============================] - 16s 516us/step - loss: 0.5288\n",
      "Epoch 77/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.5273\n",
      "Epoch 78/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.5210\n",
      "Epoch 79/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.5162\n",
      "Epoch 80/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.5152\n",
      "Epoch 81/100\n",
      "30798/30798 [==============================] - 16s 515us/step - loss: 0.5159\n",
      "Epoch 82/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.5136\n",
      "Epoch 83/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.5058\n",
      "Epoch 84/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.5019\n",
      "Epoch 85/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.4924\n",
      "Epoch 86/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.4873\n",
      "Epoch 87/100\n",
      "30798/30798 [==============================] - 16s 519us/step - loss: 0.4925\n",
      "Epoch 88/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.4830\n",
      "Epoch 89/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.4725\n",
      "Epoch 90/100\n",
      "30798/30798 [==============================] - 16s 508us/step - loss: 0.4725\n",
      "Epoch 91/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.4657\n",
      "Epoch 92/100\n",
      "30798/30798 [==============================] - 16s 507us/step - loss: 0.4636\n",
      "Epoch 93/100\n",
      "30798/30798 [==============================] - 16s 513us/step - loss: 0.4653\n",
      "Epoch 94/100\n",
      "30798/30798 [==============================] - 16s 512us/step - loss: 0.4574\n",
      "Epoch 95/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.4606\n",
      "Epoch 96/100\n",
      "30798/30798 [==============================] - 16s 516us/step - loss: 0.4539\n",
      "Epoch 97/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.4543\n",
      "Epoch 98/100\n",
      "30798/30798 [==============================] - 16s 510us/step - loss: 0.4497\n",
      "Epoch 99/100\n",
      "30798/30798 [==============================] - 16s 514us/step - loss: 0.4429\n",
      "Epoch 100/100\n",
      "30798/30798 [==============================] - 16s 511us/step - loss: 0.4301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd52207cc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop the model\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.fit(x, y, batch_size=150, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \" love asunder,\n",
      "to join with men in scorning your poor friend\"\n",
      " love asunder,\n",
      "to join with men in scorning your poor friends vill you green\n",
      "peraice, if my crowermiar:\n",
      "i will not the sun,es bull by mide fords of this enough,\n",
      "with the euds a toolence when i go sneking on merchanger sport,\n",
      "i did every so to a serve moon,\n",
      "i that comem.\n",
      "helena\n",
      "and here loved to the ear, a torarn or did them wis hounds. of the sair, he hath not her houghing her now and dream,\n",
      "man both set up hould to ading the seast fears,\n",
      "of the eyew he how dupes the seap happy fold so dead.\n",
      "exeunt becn\n",
      "ass my lord, then the fort mandyur,\n",
      "enter theseus\n",
      "the eund i do upon.\n",
      "bottom\n",
      "an i do how rull,\n",
      "of thy tendern his spolence\n",
      "must, good some can bright, of the seapons.\n",
      "pyramus\n",
      "o prove my no her so,\n",
      "with this dill for loved your corth, it do port, her on your cy?\n",
      "whitere re-ear i am tull hor of a play betulian awowled ind,\n",
      "that i, thy childst and true list bearted in:\n",
      "and facked an the seast for it with his vill bitnen tell we bearter.\n",
      "exet thencows thou look night;\n",
      "for of that you think\n",
      "tell trough the sun,e center in the same man bread the maony fell,\n",
      "tranch;\n",
      "these shall have shors here are to up, to your eyrsleght to say, to lies to the suke be they think i am to athen and someting me not.\n",
      "dottly\n",
      "here all thy can should think i see.\n",
      "hilf our comes; and true lowgithing in a despersine.\n",
      "sten fer, him the prowil will desirestants that it with phis look,\n",
      "if thou dragues;\n",
      "and but and starvels her reak as the sport of bed, of the eath sport\n",
      "were enter i must good man, that i, thou darkle commonestres,\n",
      "the eun, a promethere's shall so if her his so,\n",
      "to be artient man brim\n",
      "if her her other sail.\n",
      "helena\n",
      "the world hermia, are the foer stake my lise of the shalt with your come in her hath do a these mistrese,\n",
      "the seat\n",
      "or lith desperfect,\n",
      "i have such fer a man, hom in shors'd manhy left me as the stome and dewear.\n",
      "exeunt i, strange throu see in neit him not them till night\n",
      "of one did spuch the surmem stilether some your worl dewarls death on his ere to helenales dewermiaicht, you grow till mingriss'd it play\n",
      "you lord, i will noth to the"
     ]
    }
   ],
   "source": [
    "# Generate the text\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "sys.stdout.write(generated_text)\n",
    "for i in range(2000):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(generated_text):\n",
    "        sampled[0, t, char_indices[char]] = 1.\n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.5)\n",
    "    next_char = chars[next_index]\n",
    "    generated_text += next_char\n",
    "    generated_text = generated_text[1:]\n",
    "    sys.stdout.write(next_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
